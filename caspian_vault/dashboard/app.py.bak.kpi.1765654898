import os
import pandas as pd
import streamlit as st
import plotly.express as px
from sqlalchemy import create_engine, text

# Postgres connection (service name inside docker network is usually "postgres")
PG_HOST = os.getenv("PG_HOST", "postgres")
PG_PORT = int(os.getenv("PG_PORT", "5432"))
PG_DB   = os.getenv("PG_DB", "airflow")
PG_USER = os.getenv("PG_USER", "airflow")
PG_PASS = os.getenv("PG_PASS", "airflow")  # change if your compose uses a different password

ENGINE = create_engine(f"postgresql+psycopg2://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}")

st.set_page_config(page_title="Caspian Seismic Analytics", layout="wide")
st.title("CaspianPetro — Seismic Analytics Dashboard")

def qdf(sql: str) -> pd.DataFrame:
    with ENGINE.connect() as conn:
        return pd.read_sql(text(sql), conn)

# ---- safe format helpers (avoid NoneType crashes when tables are empty) ----
def safe_int(x, default=0):
    try:
        if x is None or pd.isna(x):
            return default
        return int(x)
    except Exception:
        return default

def safe_float(x):
    try:
        if x is None or pd.isna(x):
            return None
        return float(x)
    except Exception:
        return None

def fmt_pct(x):
    v = safe_float(x)
    return "N/A" if v is None else f"{v*100:.2f}%"

def fmt_float(x, digits=4):
    v = safe_float(x)
    return "N/A" if v is None else f"{v:.{digits}f}"


# ---- KPIs ----
kpi = qdf("""
select
  (select count(*) from public.dim_well) as wells,
  (select count(*) from public.fct_readings) as readings,
  (select avg(case when quality_flag=1 then 1 else 0 end) from public.fct_readings) as quality_rate,
  (select avg(amplitude) from public.fct_readings) as avg_amp
""")

c1, c2, c3, c4 = st.columns(4)
c1.metric("Wells", int(kpi["wells"][0]))
c2.metric("Total readings", int(kpi["readings"][0]))
c3.metric("Data quality rate", fmt_pct(kpi["quality_rate"].iloc[0] if (len(kpi) and "quality_rate" in kpi.columns) else None))
c4.metric("Avg amplitude", fmt_float(kpi["avg_amp"].iloc[0] if (len(kpi) and "avg_amp" in kpi.columns) else None, digits=4))

st.divider()

# ---- Map of wells ----
st.subheader("Wells map")
wells = qdf("select well_id, lat, lon from public.dim_well")
fig_map = px.scatter_mapbox(
    wells, lat="lat", lon="lon", hover_name="well_id", zoom=4, height=420
)
fig_map.update_layout(mapbox_style="open-street-map", margin=dict(l=0, r=0, t=0, b=0))
st.plotly_chart(fig_map, use_container_width=True)

st.divider()

# ---- Amplitude distribution ----
st.subheader("Amplitude distribution")
amp = qdf("select amplitude, quality_flag from public.fct_readings")
fig_hist = px.histogram(amp, x="amplitude", color="quality_flag", nbins=60)
st.plotly_chart(fig_hist, use_container_width=True)

# ---- Anomaly heatmap ----
st.subheader("Anomaly heatmap (depth bucket × well)")
heat = qdf("""
with stats as (
  select hk_well, avg(amplitude) as mu, stddev_samp(amplitude) as sigma
  from public.fct_readings
  group by 1
),
scored as (
  select
    f.hk_well,
    cast(floor(depth_ft/50)*50 as int) as depth_bucket,
    case
      when s.sigma is null or s.sigma = 0 then 0
      else abs(f.amplitude - s.mu) / s.sigma
    end as z
  from public.fct_readings f
  join stats s using (hk_well)
)
select
  w.well_id,
  depth_bucket,
  avg(z) as avg_z
from scored
join public.dim_well w using (hk_well)
group by 1,2
""")

pivot = heat.pivot(index="depth_bucket", columns="well_id", values="avg_z").fillna(0)
fig_hm = px.imshow(pivot, aspect="auto")
st.plotly_chart(fig_hm, use_container_width=True)

st.divider()

# ---- Marts ----
st.subheader("mart_well_performance")
m1 = qdf("""
select w.well_id, m.source_format, m.total_readings, m.avg_amplitude, m.data_quality_rate
from public.mart_well_performance m
join public.dim_well w using (hk_well)
order by total_readings desc
""")
st.dataframe(m1, use_container_width=True)

st.subheader("mart_sensor_analysis (survey_type as sensor type)")
m2 = qdf("""
select s.survey_type_id as sensor_type_id, m.total_readings, m.data_quality_rate, m.avg_amplitude
from public.mart_sensor_analysis m
join public.dim_survey_type s on s.hk_survey_type = m.hk_sensor_type
order by total_readings desc
""")
st.dataframe(m2, use_container_width=True)

st.subheader("mart_survey_summary")
m3 = qdf("""
select s.survey_type_id, m.source_format, m.wells_surveyed, m.total_readings, m.avg_amplitude,
       m.first_ingest_ts, m.last_ingest_ts
from public.mart_survey_summary m
join public.dim_survey_type s using (hk_survey_type)
order by total_readings desc
""")
st.dataframe(m3, use_container_width=True)
